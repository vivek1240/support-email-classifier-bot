{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_email_project_part_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-GCGEU6wjDB"
      },
      "source": [
        "# Support Email Classification Project part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jB6Q-pRwpvQ"
      },
      "source": [
        "## In this notebook modelling part is covered and the end model is saved as pickle file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EUBNdUzvtMo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8EESX2FvtOg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFoKIvxBvvUZ"
      },
      "source": [
        "#Importing cleaned dataset\n",
        "support_email_data_9_col_2= pd.read_csv(r'/content/drive/MyDrive/fully_cleaned_email_data.csv',encoding= 'latin-1')\n",
        "support_email_data_9_col_2= support_email_data_9_col_2[['Request_Type','cleaned_body']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJQeGM_0MT4"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhW-9RK0ZkN"
      },
      "source": [
        "### 1. Removed rows with nan values \n",
        "### 2. Removed the non-required categories(*Booking Query*, *Promotions*, *Eticket*/*Voucher*, *Booking Information*) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WSxzkMSwCaE"
      },
      "source": [
        "#Removing rows with nan  and not required categories \n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col['Request_Type'].notna()]\n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col['cleaned_body'].notna()]\n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col.Request_Type != 'Booking Query']\n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col.Request_Type != 'Promotions']\n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col.Request_Type != 'Eticket/Voucher']\n",
        "support_email_data_9_col= support_email_data_9_col[support_email_data_9_col.Request_Type != 'Booking information']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z2VW1tD01wm"
      },
      "source": [
        "##Final Categories Taken Are \n",
        "### 1. Refund \n",
        "### 2. Others\n",
        "### 3. Amendments\n",
        "### 4. Cancellation\n",
        "### 5. Website Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdlfysKbvvWr"
      },
      "source": [
        "ax = support_email_data_9_col['Request_Type'].value_counts().plot(kind='bar', figsize=(19,9), title=\"Fuck your feelings\")\n",
        "ax.set_xlabel(\"Request_Type\")\n",
        "ax.set_ylabel(\"Frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKWC4Pw41x-M"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1og3gHb11VJ"
      },
      "source": [
        "## Steps Inovolved\n",
        "### 1. Formed a pipline which will do the following processes \n",
        "### 2. Formation of Tfidf vectors \n",
        "### 3. Used Chi2 for best feature selection\n",
        "### 4. Random forest for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezj-BWa_vvZG"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sqlite3 import Error\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sqlite3\n",
        "import pickle\n",
        "import nltk\n",
        "import copy\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df=copy.deepcopy(support_email_data_9_col)\n",
        "vectorizer = TfidfVectorizer(min_df= 1,max_df= .94, stop_words=\"english\", sublinear_tf=True, norm='l1', ngram_range=(1, 1))\n",
        "final_features = vectorizer.fit_transform(df['cleaned_body']).toarray()\n",
        "final_features.shape  \n",
        "\n",
        "    \n",
        "# this block is to split the dataset into training and testing set \n",
        "X = df['cleaned_body']\n",
        "df['label_num']  =  df['Request_Type'].map({'Refunds': 0, 'Cancellation': 1,'Others': 2,'Amendment': 3, 'Website Error':4})\n",
        "#df['label_num'] = df['Request_Type'].factorize()[0] #class embedding\n",
        "Y = df['label_num']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 0, test_size=0.25)\n",
        "\n",
        "# instead of doing these steps one at a time, we can use a pipeline to complete then all at once\n",
        "pipeline = Pipeline([('vect', vectorizer),\n",
        "                     ('chi',  SelectKBest(chi2, k=1000)),\n",
        "                     ('clf', RandomForestClassifier())])\n",
        "\n",
        "# fitting our model and save it in a pickle for later use\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "with open('RandomForest.pickle', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "ytest = np.array(y_test)\n",
        "\n",
        "# confusion matrix and classification report(precision, recall, F1-score)\n",
        "print(classification_report(ytest, model.predict(X_test)))\n",
        "print(confusion_matrix(ytest, model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETTtCPD02t2C"
      },
      "source": [
        "### Saving model as a pickle file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6KoX2nivvbp"
      },
      "source": [
        "# save\n",
        "import pickle\n",
        "with open('classificationModel.pkl','wb') as f:\n",
        "    pickle.dump(model,f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSGShlo1wVNV"
      },
      "source": [
        "# load\n",
        "import pickle\n",
        "with open('classificationModel.pkl', 'rb') as f:\n",
        "  clf2 = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}